<!DOCTYPE HTML>
<!--
	Digital Identities: Design and Uses
	The Centre for Internet and Society, India
-->
<html>
	<head>
		<title>Digital Identities: Design and Uses</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<meta name="description" content="Digital Identities: Design and Uses | A project of the Centre for Internet and Society, India, supported by Omidyar Network" />
		<meta name="keywords" content="" />
		<link rel="stylesheet" href="assets/css/post.css" />
		<link rel="shortcut icon" type="image/x-icon" href="images/favicon.ico" />
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Intro -->
					<section class="intro">
						<header>
							<h1><a href="index.html">Digital Identities:<br />Design and Uses</a></h1>
							<p>A project of <a href="https://cis-india.org/" target="_blank">the Centre for Internet and Society, India</a><br /> supported by <a href="https://www.omidyar.com/" target="_blank">Omidyar Network</a></p>
						</header>
					</section>

				<!-- Section -->
					<section id="first">
						<div class="content">
							<h2>Governing ID: A Framework for Evaluation of Digital Identity</h2>
						</div>
					</section>

				<!-- Section -->
					<section>
						<header>
							<p id="meta"><strong>Janaury 22, 2020</strong></p>
							<p id="meta"><em>Research and Writing by</em> Vrinda Bhandari, Shruti Trikanad <em>and</em> Amber Sinha<br/>
							<p id="meta"><em>Review by</em> Kaliya Young, Yesha Tshering Paul<br/><em>and</em> Sunil Abraham<br/></p>
							
						</header>
						<div class="content">
							<p>As governments across the globe implement new and foundational digital identification systems (Digital ID), or modernize existing ID programs, there is an urgent need for more research and discussion about appropriate uses of Digital ID systems. This significant momentum for creating Digital ID has been accompanied with concerns about privacy, surveillance and exclusion harms of state-issued Digital IDs in several parts of the world, resulting in campaigns and litigations in countries, such as UK, India, Kenya, and Jamaica. Given the sweeping range of considerations required to evaluate Digital ID projects, it is necessary to formulate evaluation frameworks that can be used for this purpose.</p>
							<p>This work began with the question of what the appropriate uses of Digital ID can be, but through the research process, it became clear that the question of use cannot be divorced from the fundamental attributes of Digital ID systems and their governance structures. This framework provides tests, which can be used to evaluate the governance of Digital ID across jurisdictions, as well as determine whether a particular use of Digital ID is legitimate. Through three kinds of checks — Rule of Law tests, Rights based tests, and Risks based tests — this scheme is a ready guide for evaluation of Digital ID.</p>
						</div>
					</section>

				<!-- Divider -->
				<div id="divider"><hr /></div>

				<!-- Section -->
					<section>
						<header>
							<h3>Rule of Law Tests</h3>
						</header>
						<div class="content">
							<p>The rise of Digital ID, and the opportunities they present, both for public and private actors, have in the past resulted in hasteful implementations and adoptions. This does not allow for sufficient deliberation to lead to governance mechanisms. Below are the most <em>basic</em> tests to ensure that a rule of law framework exists to govern the use of ID —</p>
						</div>
					</section>

				<!-- Section -->
					<section>
						<header>
							<h3>1.1</h3>
						</header>
						<div class="content">
							<h5>Legislative Mandate</h5>
							<h4 style="margin-bottom: 0.75em; font-size: 1.5em; line-height: 1.2em;">Is the project backed by a validly enacted law? Does the law amount to excessive delegation?</h4>
							<p>Digital ID, by its nature, will entail greater collection and generation of personally identifiable information, and privacy risks, which arise from it. Any such restrictions to the fundamental right to privacy must be prescribed by law in the form of a publicly available legislative act. Other forms of regulation, such as executive ordinance, only meet this requirement in limited ways. </p>
							<p class="indent">A validly enacted law has three components: (i) it should be passed by the Legislature, and not the Executive<sup class="superscript"><a href="#fn1">1</a></sup><a name="ref1"></a>&thinsp;; (ii) it should be accessible and foreseeable — this is to ensure the ‘quality of law’; and (iii) it should be clear and precise — this is to limit the scope of discretion. Each of these three legal requirements is explained in some detail below —</p>
						</div>
					</section>

					<section>
						<header>
							<h4>(i)</h4>
						</header>
						<div class="content">
							<h4>Legality</h4><br/>
							<p>By its very nature, the collection, storage, and use of personally identifiable information through a Digital ID — especially if it has any mandatory requirements of identification, authentication, or authorisation — is likely to violate the right to privacy of the individual and affect their right to free speech, particularly as it leads to a chilling effect.<sup class="superscript"><a href="#fn2">2</a></sup><a name="ref2"></a> These concerns are exacerbated if the Digital ID is meant as a single online identity, that will more or less replace the use of existing functional identities, as was the case with several national identity programmes. </p>
							<p class="indent">The rule of law requires that every act by the State or by its officers must, if it is to operate to the prejudice of any person, be supported by some legislative authority.<sup class="superscript"><a href="#fn3">3</a></sup><a name="ref3"></a> There should be ‘a law,’ having statutory force and not a mere executive or departmental instruction.<sup class="superscript"><a href="#fn4">4</a></sup><a name="ref4"></a></p>
							<p class="indent">Thus, to pass constitutional muster, this infringement of a fundamental right or the invasion of life and personal liberty must be sanctioned by a law, having statutory force, enacted by the appropriate legislative body. This is the first prong of the proportionality test, as has been accepted key jurisdictions such as India,<sup class="superscript"><a href="#fn5">5</a></sup><a name="ref5"></a> UK,<sup class="superscript"><a href="#fn6">6</a></sup><a name="ref6"></a>  Canada,<sup class="superscript"><a href="#fn7">7</a></sup><a name="ref7"></a> South Africa,<sup class="superscript"><a href="#fn8">8</a></sup><a name="ref8"></a> and by the European Court of Human Rights (‘ECtHR’) (as part of Article 8(2), European Charter of Human Rights’ requirement of ‘accordance with law’).<sup class="superscript"><a href="#fn9">9</a></sup><a name="ref9"></a></p>
						</div>
					</section>

					<section>
						<header>
							<h4>(ii)</h4>
						</header>
						<div class="content">
							<h4>Quality of Law</h4><br/>
							<p>Some courts have interpreted the legal standard of ‘in accordance with law’ as requiring ‘quality of law’, namely that the law is “accessible to the person concerned, who must, moreover, be able to foresee its consequences for him, and compatible with the rule of law.”&thinsp;<sup class="superscript"><a href="#fn10">10</a></sup><a name="ref10"></a> In this definition, foreseeability would require that the law be “sufficiently clear in its terms to give citizens an adequate indication of the conditions and circumstances in which the authorities are empowered to resort to any such measures”.<sup class="superscript"><a href="#fn11">11</a></sup><a name="ref11"></a> While absolute certainty is not required as part of the ‘foreseeability’ requirement,<sup class="superscript"><a href="#fn12">12</a></sup><a name="ref12"></a> the rationale behind introducing these requirements is to prevent any arbitrary interference with fundamental rights by the State.<sup class="superscript"><a href="#fn13">13</a></sup><a name="ref13"></a></p>
						</div>
					</section>

					<section>
						<header>
							<h4>(iii)</h4>
						</header>
						<div class="content">
							<h4>Clarity and Precision of Law</h4><br/>
							<p>The requirement of clarity and precision aims to make the law specific, so as to limit its scope. By regulating the exercise of discretion, it serves as an effective guarantee against abuse.<sup class="superscript"><a href="#fn14">14</a></sup><a name="ref14"></a></p>
							<p class="indent">In this context, the relevant factors to ensure legislative quality are the quality of the substantive content of the law governing Digital ID, its form and language, the manner in which it was implemented, and its ‘effectiveness’, i.e. its ability to produce the desired regulatory results.<sup class="superscript"><a href="#fn15">15</a></sup><a name="ref15"></a></p>
						</div>
					</section>

					<section>
						<div class="content">
							<p>Further, it is a settled principle that a law is void if it is vague,<sup class="superscript"><a href="#fn16">16</a></sup><a name="ref16"></a> and a vague law is one which impermissibly or excessively delegates basic policy matters to the executive, or if the Legislature abdicate its duty of laying down adequate guidelines for the exercise of Executive power.<sup class="superscript"><a href="#fn17">17</a></sup><a name="ref17"></a> This can lead to arbitrary and discriminatory application of the law. In the context of a Digital ID law, the factors to consider are whether basic policy matters of collection, storage, use, and sharing of the personally-identifiable information have been delegated to a rule-making body that is part of the Executive.</p>
						</div>
					</section>

				<!-- Section -->
					<section>
						<header>
							<h3>1.2</h3>
						</header>
						<div class="content">
							<h5>Legitimate Aim</h5>
							<h4 style="margin-bottom: 0.75em; font-size: 1.5em; line-height: 1.2em;">Does the law have a ‘legitimate aim?’ Are all purposes flowing from a ‘legitimate aim’ identified in the valid law?</h4>
							<p>All the purposes for use of Digital ID thus, must correspond to a legitimate aim identified in the valid law.<sup class="superscript"><a href="#fn18">18</a></sup><a name="ref18"></a> The ECtHR has held that this legitimate aim should be “necessary in a democratic society,”&thinsp;<sup class="superscript"><a href="#fn19">19</a></sup><a name="ref19"></a> i.e., it must answer a “pressing social need.” It should not based merely on political expediency.<sup class="superscript"><a href="#fn20">20</a></sup><a name="ref20"></a></p>
							<p class="indent">In the context of a Digital ID, some illustrations of a ‘legitimate aim’ may be “in the interests of national security, public safety or the economic well-being of the country, for the prevention of disorder or crime, for the protection of health or morals, or for the protection of the rights and freedoms of others.”&thinsp;<sup class="superscript"><a href="#fn21">21</a></sup><a name="ref21"></a> The burden of proof in such cases, must be on the State to demonstrate the legitimate aim of the proposed law.<sup class="superscript"><a href="#fn22">22</a></sup><a name="ref22"></a> The only overarching requirement of the ‘legitimate aim’ standard is that it should not operate in a manner that discriminates on the basis of race, colour, sex, language, religion, political or other opinion, national or social origin, property, birth or other status.<sup class="superscript"><a href="#fn23">23</a></sup><a name="ref23"></a></p>
							<p class="indent">However, the legitimate aim is only one part of the proportionality test; any law governing Digital ID will still have to pass the proportionality test, which will be discussed below.</p>
						</div>
					</section>


				<!-- Section -->
					<section>
						<header>
							<h3>1.3</h3>
						</header>
						<div class="content">
							<h5>Defining Actors and Purposes</h5>
							<h4 style="margin-bottom: 0.75em; font-size: 1.5em; line-height: 1.2em;">Does the law clearly specify the actors and the purposes that would flow from the legitimate aim?</h4>
							<p>The legitimate aims for Digital ID must be identified in the law governing the project. Key vectors for determining the legitimacy of aims for such projects are the actors who use Digital ID, and the purposes for which Digital ID must be used.</p>
						</div>
					</section>

					<section>
						<header>
							<h4>(i)</h4>
						</header>
						<div class="content">
							<h4>Actors</h4><br/>
							<p>The law must clearly specify the actors, or a category of actors who may use the Digital ID. Actors include both the State and private actors; entities who may use the Digital ID, and agencies and databases to whom it may be connected in any way.</p>
							<p class="indent">Privacy serves as a restraint on the power of the government and private entities.<sup class="superscript"><a href="#fn24">24</a></sup><a name="ref24"></a> Consequently, the Digital ID law would also have to clarify whether — keeping in mind the legitimate aim of the Digital ID and whether it was mandating the use of the ID to access any service — it would apply <em>equally</em> to State and private actors.</p>
							<p class="indent">There has been limited examination of Digital ID by the courts, keeping in mind these principles. For instance, in India, in <em>K.S. Puttaswamy v Union of India (II)</em>&thinsp;<sup class="superscript"><a href="#fn25">25</a></sup><a name="ref25"></a> (“Aadhaar Judgment”) the court broadly struck down the use of the Digital ID programme by the private sector as being unconstitutional and disproportionate, enabling ‘commercial exploitation’ of biometric and demographic information by private parties, and due to concerns of possible profiling.<sup class="superscript"><a href="#fn26">26</a></sup><a name="ref26"></a> <sup class="superscript"><a href="#fn27">27</a></sup><a name="ref27"></a> It is worth noting that more than the focus of private and public actors, how digital identity is used is important. While we have had limited experience with digital identification systems, private use of other identification programmes has also been regulated. In the US, states such as Alaska, Kansas, Maine, New Mexico and Rhode Island either restrict the solicitation of Social Security Numbers or prohibit denying goods and services to an individual who declines to give their Social Security Number by private parties.<sup class="superscript"><a href="#fn28">28</a></sup><a name="ref28"></a> </p>
							<p class="indent">In light of the unresolved questions around private sector involvement in Digital ID, we believe that any law governing Digital ID should make clear (i) if and how private entities can use the Digital ID infrastructure created by the State? (ii) if so, can they mandate the use of a Digital ID to get access to private services such as banking and telecom? (iii) is the private sector’s use of the Digital ID and its surrounding infrastructure fulfilling the legitimate aim of the law and the legitimate purpose of using the ID? and (iv) if private sector entities will be held to the same standards of accountability as the State.</p>
						</div>
					</section>

					<section>
						<header>
							<h4>(ii)</h4>
						</header>
						<div class="content">
							<h4>Purposes</h4><br/>
							<p>Similarly, the purposes or the category of purposes for which the Digital ID is used must always be backed by law and clearly and explicitly defined.<sup class="superscript"><a href="#fn29">29</a></sup><a name="ref29"></a> In a common law country, this can be done in the Statement of Objects and Reasons behind introducing the law, in the Notes and Clauses of the individual provisions of the law, or in the ministerial speech moving the law in legislature.</p>
							<p class="indent">The data collected by the State for the fulfilment of the legitimate State aim must be used to fulfil only those legitimate purposes that flow from the said aim, the burden of proving which is upon the State.<sup class="superscript"><a href="#fn30">30</a></sup><a name="ref30"></a> The nature of data required to fulfill this legitimate aim must also be expressly specified. The Digital ID should not be used for any extraneous or unauthorised purposes,<sup class="superscript"><a href="#fn31">31</a></sup><a name="ref31"></a> such as surveillance.</p>
							<p class="indent">A clearly defined purpose limitation<sup class="superscript"><a href="#fn32">32</a></sup><a name="ref32"></a> of the Digital ID will allows users to limit the collection and retention of personal data to what is ‘strictly necessary’ and exercise their rights to object to any processing that is not considered ‘strictly necessary.’&thinsp;<sup class="superscript"><a href="#fn33">33</a></sup><a name="ref33"></a> It will also prevent expansion of the project by way of mission creep.</p>
							<p class="indent">In the context of a Digital ID, for instance, if the aim of the ID is to authenticate users for the provision of services, the Legislature should consider whether it is necessary to collect the biometric information of an individual (or if demographic information would suffice), and if so, should this biometric information include fingerprints or DNA samples? The Digital ID law should also clarify whether the sensitive personal information collected for one purpose (e.g. for provision of benefits) can be used for an entirely <em>unrelated</em> purpose (e.g. SIM card authentication). Ideally, it should not, and any change of purpose/new purpose should be notified to the data subject for fresh consent.<sup class="superscript"><a href="#fn34">34</a></sup><a name="ref34"></a> Finally, a well defined purpose will enable citizens to determine whether centralised storage or long term retention of their data is necessary to achieve the purpose of the national Digital ID law.<sup class="superscript"><a href="#fn35">35</a></sup><a name="ref35"></a></p>
						</div>
					</section>



				<!-- Section -->
					<section>
						<header>
							<h3>1.4</h3>
						</header>
						<div class="content">
							<h5>Redressal Mechanism</h5>
							<h4 style="margin-bottom: 0.75em; font-size: 1.5em; line-height: 1.2em;">Does the law provide for adequate redressal mechanisms against actors who use the Digital ID and govern its use?</h4>
							<p>Adequate redressal mechanisms would necessarily include the following three requirements —</p>
						</div>
					</section>

					<section>
						<header>
							<h4>(i)</h4>
						</header>
						<div class="content">
							<h4>User Notification</h4><br/>
							<p>Individuals must be notified or at least be able to access information on when their Digital ID is used in any way, e.g. during every authentication procedure. This will allow citizens to be informed of every instance of usage of their personal data, as is with the case of credit and debit cards.<sup class="superscript"><a href="#fn36">36</a></sup><a name="ref36"></a></p>
							<p class="indent">There should also be proactive notification when there is a breach of their data. A national ID system, by its very nature, will be collecting and storing large swathes of sensitive and personal demographic, and possibly biometric information of the country’s residents/citizens. The possibility of data breach thus, cannot be ruled out, and is especially dangerous when permanent identifiers like biometrics are used (which is another reason why biometrics should ideally not be a part of the Digital ID system). Hence, the Digital ID law should put in place legal requirements for the affected agency/data controller to notify the affected residents, as soon as a data breach occurs, and explain to them the impact of this breach.<sup class="superscript"><a href="#fn37">37</a></sup><a name="ref37"></a> This also ties in with the principle of accountability.</p>
						</div>
					</section>

					<section>
						<header>
							<h4>(ii)</h4>
						</header>
						<div class="content">
							<h4>Access and Correction</h4><br/>
							<p>The rights to access and correction are derived from the citizens’ right to know, including their right to receive information, which are a part of the right to freedom of speech and expression.<sup class="superscript"><a href="#fn38">38</a></sup><a name="ref38"></a></p>
							<p class="indent">Individuals must have the right to access personally identifiable information collected through the use of Digital ID, to be able to confirm the data being held by the data controller, and to be able to obtain a copy of the same.<sup class="superscript"><a href="#fn39">39</a></sup><a name="ref39"></a> In the context of Digital IDs, this will enable the citizens to know the different agencies that have access to, and are able to process their digital ID information, including their biometric information. They should also have the ability to seek corrections, amendments, or deletion of such information where it is inaccurate.<sup class="superscript"><a href="#fn40">40</a></sup><a name="ref40"></a></p>
						</div>
					</section>

					<section>
						<header>
							<h4>(iii)</h4>
						</header>
						<div class="content">
							<h4>Due Process</h4><br/>
							<p>A Digital ID law should have a well-designed grievance redress framework that addresses concerns of accountability, transparency, and user-friendliness.<sup class="superscript"><a href="#fn41">41</a></sup><a name="ref41"></a></p>
							<p class="indent">Individuals must be entitled to a fair and public hearing within a reasonable time by an independent, competent, and impartial judicial authority that is established by law, in cases where provisions of law governing the Digital ID are violated. This should take the form of adequate civil and criminal grievance redress mechanisms, with separate procedures for appeal.  Appropriate remedies for damage caused due to violations or errors must be accounted for, including in the form of monetary compensation where necessary.</p>
							<p class="indent">Civil redress mechanisms will have to be set up to deal with issues of omission or deactivation of the Digital ID (due to provision of false information or non-use), errors in the enrolment and verification process, or when there is an authentication failure (to prevent exclusion). All these acts have serious civil consequences of excluding citizens from the benefits of the Digital ID system, and thus, any adverse act (such as deactivating the ID, rejecting the enrolment, or denying benefits due to authentication failure) should be preceded by following due process and giving the aggrieved citizen a proper hearing.<sup class="superscript"><a href="#fn42">42</a></sup><a name="ref42"></a> It is also important that both civil and criminal redressal proceedings can be initiated by both regulators responsible for governance of Digital ID, as well as individuals — or class of individuals — who may be impacted.<sup class="superscript"><a href="#fn43">43</a></sup><a name="ref43"></a></p>
						</div>
					</section>

				<!-- Section -->
				
					<section>
						<header>
							<h3>1.5</h3>
						</header>
						<div class="content">
							<h5>Accountability</h5>
							<h4 style="margin-bottom: 0.75em; font-size: 1.5em; line-height: 1.2em;">Are there adequate systems for accountability of governing bodies, users of Digital ID and other actors?</h4>
							<p>The collection, storage, and use of sensitive and personal information occasions a duty of care for its protection.<sup class="superscript"><a href="#fn44">44</a></sup><a name="ref44"></a> The laws governing Digital ID must provide for systems of accountability for the bodies that implement and operate the Digital ID, regulators, public and private actors which use Digital ID in any way, and other enabling or supporting actors.</p>
							<p class="indent">The principle of accountability is a well-recognised privacy principle.<sup class="superscript"><a href="#fn45">45</a></sup><a name="ref45"></a> However, it is important to understand that accountability does not replace existing law, nor does it redefine privacy. Instead, it merely seeks to improve <em>privacy governance</em> and ensure effective compliance with existing laws to achieve the law’s privacy objectives.<sup class="superscript"><a href="#fn46">46</a></sup><a name="ref46"></a> Given the vast enterprise of data collection, storage, and use that is carried out by the government and possibly private actors, individuals often have little knowledge or control over their personal data. Accountability thus assumes an even greater role in this context.</p>
							<p class="indent">Accountability can be both <em>ex-ante</em> and <em>ex-post</em>,<sup class="superscript"><a href="#fn47">47</a></sup><a name="ref47"></a> and achieved through a variety of ways — better enforcement of laws, an effective regulatory framework, a proper delineation of responsibility amongst the various actors in the Digital ID system, transparency, user breach notification, and efficient grievance redress procedures.</p>
							<p class="indent">In a Digital ID system, it can also take the form of requiring the Digital ID agency to maintain an access log (tracking who accessed the data, when, where, and for what purpose) that is associated with the identity for the user to consult at any time.<sup class="superscript"><a href="#fn48">48</a></sup><a name="ref48"></a> This will help prevent any misuse. If such logs are maintained, unauthorized access must be especially guarded against, as the logs, together with the metadata they generate, enable major inferences to be made about an individual. To that end, there must be a process to periodically delete or anonymise older logs. A legislation governing Digital ID should ideally also separate the role of the administrator, who is in charge of the storage of personal data and regulator, who licenses other agencies to perform enrolment and authentication functions and is in charge of grievance redress of the Digital ID program. There is an inherent conflict of interest if the same body performs both roles.<sup class="superscript"><a href="#fn49">49</a></sup><a name="ref49"></a> Thus, the Digital ID law should set up an <em>independent</em> and robust regulatory or monitoring framework, so that the administrator of the Digital ID can be held responsible for any breach in the database.</p>
						</div>
					</section>
				
				
				<!-- Section -->
					<section>
						<header>
							<h3>1.6</h3>
						</header>
						<div class="content">
							<h5>Mission Creep</h5>
							<h4 style="margin-bottom: 0.75em; font-size: 1.5em; line-height: 1.2em;">Is there a legislative and judicial oversight mechanism to deal with cases of mission creep in the use of Digital ID?</h4>
							<p>Mission creep or function creep is the idea that a system or technology that has been developed for one purpose, or with one set of capabilities, ends up getting used for other purposes it was not originally intended for. These subsequent uses may be advantageous, but more often than not, are pernicious, and include profiling and surveillance.<sup class="superscript"><a href="#fn50">50</a></sup><a name="ref50"></a> Mission creep or function creep thus, ends up conflicting with the right to privacy.</p>
							<p class="indent">Mission creep takes place on the back of data collected/generated through the use of a Digital ID. It is particularly problematic when the Digital ID captures biometric information, stores it centrally, and then uses it across different services (e.g. identification for a drivers’ license to authentication for receipt of benefits), since the mission creep raises heightened concerns of surveillance.<sup class="superscript"><a href="#fn51">51</a></sup><a name="ref51"></a></p>
							<p class="indent">As time progresses, Digital ID systems have a greater probability of suffering from mission creep. To prevent mission creep, the governing Act must explicitly specify the proposed uses of the Digital ID and the particular data being collected. Further, it is important to explain to the citizens the nature of their personal information being collected, the purpose it will be used for, and the agency using it. This is also in line with the traditional notice and consent framework and the idea of informational self-determination. Thereafter, the executive authority must not be able to allow for attempts to use the Digital ID for newer purposes unless there is a proper legislative process for deliberating the additional uses, or a judicial examination of these uses against the legitimate aims, or a fresh consent sought from the citizens.</p>
							<p class="indent">Some of the recognised mechanisms to limit mission creep are<sup class="superscript"><a href="#fn52">52</a></sup><a name="ref52"></a> —</p>
							<p style="padding: 1em; font-style: italic;">“(i) limiting the amount of data that is collected for any stated purpose; (ii) enabling regulation to limit technological access to the system; (iii) concerted debates with all stakeholders and public participation; (iv) dispersion of multiple enablers for a system; and (v) enabling choices for user participation.”</p>
							<p>The above strategies if specifically included in the law governing Digital ID, can help ensure that sensitive and personal information collected for one purpose does not end up getting used for another, unintended purpose. This will aid in constraining government power.</p>
						</div>
					</section>
				

				<!-- Divider -->
				<div id="divider"><hr /></div>

				<!-- Section -->
					<section>
						<header>
							<h3>Rights based Tests</h3>
						</header>
						<div class="content">
							<p>The most clear and outright critiques of Digital ID systems have come in light of their violations of the right to privacy. Across jurisdictions, critics have discussed different forms of violations of privacy, including mandatory collection of sensitive personal data such as biometrics, lack of robust access-control mechanisms, inadequate protection of private sector collection of data, and increased behavioral profiling through use of one identifier for all services. Alongside, there have also been serious questions raised about exclusion concerns where absence of an ID or failures in its functioning can lead to denial of basic entitlements and benefits. Key rights-based principles are highlighted below —</p>
						</div>
					</section>

				<!-- Section -->
					<section>
						<header>
							<h3>2.1</h3>
						</header>
						<div class="content">
							<h5>Necessary and Proportionate</h5>
							<h4 style="margin-bottom: 0.75em; font-size: 1.5em; line-height: 1.2em;">Are the privacy violations arising from the use of Digital ID necessary and proportionate to achieve the legitimate aim?</h4>

							<p>The use of Digital ID may pose inherent risks to the right to privacy by leading to generation of more data, facilitating the connection of varied sets of behavioral data to unique identities, enabling greater surveillance, and involving new sets of actors. It is now well settled that the <em>mere</em> storage and retention of personal data for unspecified purpose or without regard for informed consent is a violation of the right to privacy.<sup class="superscript"><a href="#fn53">53</a></sup><a name="ref53"></a> The subsequent use or abuse of this stored information has no bearing on this finding.<sup class="superscript"><a href="#fn54">54</a></sup><a name="ref54"></a></p>

							<p class="indent">Given that privacy is not an absolute right, it is important to determine whether restrictions on this right are just, fair, and reasonable; and whether, on this basis, the Digital ID can be sustained.<sup class="superscript"><a href="#fn55">55</a></sup><a name="ref55"></a> Privacy violations arising from the use of Digital ID must satisfy the requirement of proportionality.</p>

							<p class="indent">Under ECtHR law, an interference with the right to privacy is sustained if it is  ‘necessary in a democratic society’, which, <em>inter alia</em>, requires it to be proportionate to the legitimate aim pursued, and the reasons adduced by the national authorities to justify it must be ‘relevant and sufficient.’&thinsp;<sup class="superscript"><a href="#fn56">56</a></sup><a name="ref56"></a> Differing standards of burden of proof and margin of appreciation have been applied in proportionality inquiries under Article 8, ranging from a ‘priority to rights’ to ‘balance’ between the rights and exceptions.<sup class="superscript"><a href="#fn57">57</a></sup><a name="ref57"></a></p>

							<p class="indent">In <em>MK v France</em>, the ECtHR held that the collection and retention of fingerprints of persons who had been accused, but not convicted of offences, on the grounds that it would ‘rule out’ their involvement in case someone tried to steal their identity ‘would in practice be tantamount to justifying the storage of information on the whole population of France, which would most definitely be excessive and irrelevant.’&thinsp;<sup class="superscript"><a href="#fn58">58</a></sup><a name="ref58"></a></p>

							<p class="indent">Under Indian law, violations of the right to privacy are justified if (i) they are pursuant to an existing law; (ii) there is a legitimate State aim; (iii) the proposed measure is proportional or bears a ‘rational nexus’ between the objects and means adopted to achieve the stated aim; and (iv) there are procedural guarantees against the abuse of State incursions into privacy.<sup class="superscript"><a href="#fn59">59</a></sup><a name="ref59"></a></p>

							<p class="indent">The content of the proportionality prong has been further elaborated as comprising (i) a ‘legitimate goal’ or proper purpose; (ii) ‘suitability’, namely that the law must be a suitable means of furthering the aforesaid legitimate goal; (iii) ‘necessity’, i.e. there must not be any less restrictive but equally effective alternative present; and (iv) ‘balancing’, since the measure must not have a disproportionate impact on the right holder.<sup class="superscript"><a href="#fn60">60</a></sup><a name="ref60"></a></p>

							<p class="indent">The Jamaican Supreme Court, relying on the Canadian proportionality decision in <em>R v Oakes</em>&thinsp;<sup class="superscript"><a href="#fn61">61</a></sup><a name="ref61"></a> held that courts must take proper cognizance of ‘any deleterious effect’ of the proposed measure being pushed by the government to meet its objectives and that “greater the severity of the effect the more important the objective must be, furthermore the measure chosen needs to be shown to be the least harmful means of achieving the objective.”&thinsp;<sup class="superscript"><a href="#fn62">62</a></sup><a name="ref62"></a></p>

							<p class="indent">For a Digital ID, the proportionality inquiry would take place at various parts of the project, starting from compulsory enrolment, if any; centralised retention and storage of sensitive, personal information; and the use of the Digital ID without following purpose limitation principles or putting in place procedural safeguards. This analysis would also include determining whether an ID system is needed when its risks are grave; based on the needs and interests of the country. For instance, if a country intending to use an ID system to deliver subsidies and benefits, has a high occurrence of corruption in the middlemen delivering such services, and a large population that depends on government payments, then it may be able to justify a more invasive Digital ID. Here, societal interests, in terms of delivering essential benefits to a wide population, are high, and thus the proportionality test favours allowing an invasion of privacy of individuals. On the other hand, in a country where a smaller population depends on government benefits, and corruption amongst middlemen is scarce, then a Digital ID system with large privacy concerns may be harder to justify, and the legitimate aims of the State could instead be met by other means.</p>

							<p class="indent">At each part, the law would have to satisfy the aforestated four requirements, including demonstrating that the Digital ID is the least restrictive method of achieving the government’s stated goal. A proportionality inquiry will thus, take the form of examining whether the State could prove that <em>voluntary</em> enrolment would preclude it from achieving its stated goal of providing reliable identification.<sup class="superscript"><a href="#fn63">63</a></sup><a name="ref63"></a> It would also attach itself to questions about whether the State has failed to destroy the excess identity information collected; or whether it has enacted safeguards to ensure access control and purpose limitation.<sup class="superscript"><a href="#fn64">64</a></sup><a name="ref64"></a></p>
						</div>
					</section>

				<!-- Section -->
					<section>
						<header>
							<h3>2.2</h3>
						</header>
						<div class="content">
							<h5>Data Minimisation</h5>
							<h4 style="margin-bottom: 0.75em; font-size: 1.5em; line-height: 1.2em;">Are there clear limitations on what data may be collected, how it may be processed and how long it is retained for, during the use of Digital ID?</h4>
							<p>As uses of Digital ID emerge, there will be a need to exercise the principles of data minimisation to ensure that only data which is necessary is collected; this data is processed only for legitimate purposes; and is stored for only as long as necessary.<sup class="superscript"><a href="#fn65">65</a></sup><a name="ref65"></a> This will help mitigate the harm caused by a possible data breach or identity theft,<sup class="superscript"><a href="#fn66">66</a></sup><a name="ref66"></a> and reduce possible abuse of State power.<sup class="superscript"><a href="#fn67">67</a></sup><a name="ref67"></a></p>

							<p class="indent">Data minimisation has been defined as the ‘practice of limiting the collection of personal information to that which is necessary to accomplish a specified purpose.’&thinsp;<sup class="superscript"><a href="#fn68">68</a></sup><a name="ref68"></a> Article 5 of the GDPR requires the processing of personal data to be ‘adequate, relevant and limited to what is necessary’. We take a broad view of data minimisation as underpinning the principles of collection limitation, storage limitation/retention, use limitation and purpose specification.<sup class="superscript"><a href="#fn69">69</a></sup><a name="ref69"></a> After the purpose for which the sensitive and personal information has been collected is fulfilled, the data controller/administrator should delete this data permanently from its records.</p>

							<p class="indent">Thus, data minimization will dictate the amount and type of information that needs to be collected and stored for a Digital ID. It also requires that the storage and retention of data be proportionate. The administrator of the Digital ID has to determine, for instance, whether the collection of biometric or health information is necessary for achieving the purpose of the Digital ID; or whether the automatic storage (instead of deletion) of all the metadata relating to an authentication transaction is proportionate. The ECtHR has held that the permanent retention of fingerprint and DNA of persons who are <em>suspects</em> in <em>any</em> crime, but have not yet been convicted is ‘blanket and indiscriminate’ to the object of crime prevention and detection.<sup class="superscript"><a href="#fn70">70</a></sup><a name="ref70"></a></p>

						</div>
					</section>
				
				<!-- Section -->
				
					<section>
						<header>
							<h3>2.3</h3>
						</header>
						<div class="content">
							<h5>Access Control</h5>
							<h4 style="margin-bottom: 0.75em; font-size: 1.5em; line-height: 1.2em;">Are there protections in place to limit access to the digital trail of personally identifiable information created through the use of Digital ID by both state and private actors?</h4>

							<p>Privacy risks to individuals from use of Digital ID arise both from generation of data, as well as access to the generated data. Therefore, adequate access control mechanisms would entail regulation of access to information generated as a result of the use of Digital ID, and limiting the actors who need, and are authorised, to use this information to achieve the specified purposes of the Digital IDs. Access control measures need to be introduced within the Digital ID law, or at the very least in the general data protection framework.</p>

							<p class="indent">While determining access control, the law would have to consider the desired extent of private and governmental access to sensitive and personal information attached with the Digital ID, and whether different standards would apply to them. It would also have to take into consideration whether the personal data is being stored in a centralised or federated database, the time period of retention, and whether the Digital ID is being linked to multiple State and private databases. A Digital ID law should also clarify whether sharing of personal data and metadata (relating to the authentication transactions) amongst various agencies is permitted, and if so, should include specific rules (e.g. on consent to be taken of the Digital ID holder) to govern such practices.</p>

							<p class="indent">One way of achieving access control is to prevent seeding, especially inorganic seeding. Seeding is the  process by which the unique identification number associated with the Digital ID is seeded across various databases – the linking/merging of these different silos of data facilitates easier tracking, profiling, and surveillance.</p>

							<p class="indent">Access control thus, becomes essential, and can only be properly enforced if there is an effective and efficient right to legal recourse, with strict civil and criminal penalties for acts such as accessing, using, publishing, or sharing information in the Digital ID with third parties/online.</p>
						</div>
					</section>
				
				<!-- Section -->
					<section>
						<header>
							<h3>2.4</h3>
						</header>
						<div class="content">
							<h5>Exclusions</h5>
							<h4 style="margin-bottom: 0.75em; font-size: 1.5em; line-height: 1.2em;">Are there adequate mechanisms to ensure that the adoption of Digital ID does not lead to exclusion or restriction of access to entitlements or services?</h4>

							<p>The experience with Digital ID, particularly in the developing world, has revealed various causes for exclusion,<sup class="superscript"><a href="#fn71">71</a></sup><a name="ref71"></a> including due to poor internet connectivity,<sup class="superscript"><a href="#fn72">72</a></sup><a name="ref72"></a> fingerprints wearing out with age and manual labour,<sup class="superscript"><a href="#fn73">73</a></sup><a name="ref73"></a> disability,<sup class="superscript"><a href="#fn74">74</a></sup><a name="ref74"></a> and problems with the Point of Sale machines.<sup class="superscript"><a href="#fn75">75</a></sup><a name="ref75"></a> This proves that the exclusion is not simply a result of poor implementation, but rather, is a function of the design of the ID system, if for instance, it relies on the inherently probabilistic nature of biometrics.</p>

							<p class="indent">If the intended use of ID could lead to denial or restriction of services or benefits to individuals, or categories of individuals, then there must be mechanisms to ensure that such individuals are not disadvantaged. It is important to note that this exclusion can occur even at the stage of identification, if the costs of obtaining a Digital ID make them inaccessible to some citizens; this can be in terms of the location of identification services, typically restricted to populated areas, or in terms of requiring complex documentation, etc.</p>

							<p class="indent">Primarily, the law should not require <em>mandatory</em> authentication of the Digital ID to receive various benefits. As stated above, a low incidence of exclusion is inevitable, and will likely hit the marginalised sections of society the hardest. Thus, any Digital ID system should avoid prescribing mandatory use and should instead, provide alternative mechanisms for establishing their identity. In these cases, individuals must be able to use other forms of identification to seek access to services or benefits. Further, the Digital ID should facilitate offline and localised verification of the demographic or biometric identity.<sup class="superscript"><a href="#fn76">76</a></sup><a name="ref76"></a> Exclusion can also be avoided by ensuring data quality and accuracy, to ensure that there are no errors in the information collected and stored on the Digital ID.<sup class="superscript"><a href="#fn77">77</a></sup><a name="ref77"></a> Administrative procedure decisions involving location, language, and costs to obtain an ID could also be exclusionary factors that should be accounted for.</p>

							<p class="indent">The law should also provide for a grievance redress mechanism, following the principles of natural justice, where the aggrieved Digital ID holder (who has experienced authentication failure) is given a right to be heard. The administrator of the Digital ID must be held accountable for any failures in the Digital ID system, which can be achieved partly through judicial/independent oversight.</p>

						</div>
					</section>

				<!-- Section -->
					<section>
						<header>
							<h3>2.5</h3>
						</header>
						<div class="content">
							<h5>Mandatory Use</h5>
							<h4 style="margin-bottom: 0.75em; font-size: 1.5em; line-height: 1.2em;">In case enrolment and use of Digital ID are made mandatory, are there any valid legal grounds for doing so?</h4>

							<p>Whether enrolling into and specific uses of ID should be mandatory or not remains one of the most important questions in ID. An identity can be mandated in two forms. First, enrolment in a national ID program, as in Jamaica, where the National Identification and Registration Act (‘NIRA’) mandated the collection of biometric information from all Jamaican residents, at the pain of criminal sanction.<sup class="superscript"><a href="#fn78">78</a></sup><a name="ref78"></a> Second, mandating its use for the provision of certain services. For instance, in India, although enrolment in the identity program is not mandatory, every resident who pays taxes in India requires it. In effect, thus, it is mandatory for all tax-payers.</p>

							<p class="indent">Even assuming a legitimate aim and limited purpose of requesting certain personal information (including biometric information), the government should always seek to provide an option amongst multiple forms of identities.<sup class="superscript"><a href="#fn79">79</a></sup><a name="ref79"></a> This option should extend to an opt out mechanism, with the individual retaining its access to the service (provided they produce an alternative form of ID), and with mandatory erasure of all collected personal information from the ID system.</p>

							<p class="indent">In the NIRA Case (<em>‘Robinson’</em>), the Jamaican Supreme Court held that the compulsion and deprivation of choice inherent in the mandatory collection of biometric information engaged the liberty interest of individuals and amounted to bodily interference. In the absence of any ‘strong justification’ for the absence of an opt-out provision, the mandatory collection provision (Section 20) was deemed not justifiable in a free and democratic society and thus, declared unconstitutional. The risk of misuse/abuse by the State and the deprivation of personal choice were held to outweigh ‘any conceivable [public] benefit.’&thinsp;<sup class="superscript"><a href="#fn80">80</a></sup><a name="ref80"></a></p>

							<p class="indent">Various benefits and services provided by the States are not State largesse or ‘gifts’ to citizens of a country. However, making access to these benefits contingent on the use and authorisation of only one form of Digital ID violates these citizens rights to choose how to identify themselves to the government in a reasonable and non-intrusive fashion.</p>

							<p class="indent">Thus, keeping in view the importance of choice, consent, dignity, and informational self determination, enrolment in a Digital ID should not be made mandatory. This is especially true in the case of children. An identity that is made mandatory should be subject to strict legal tests, such as the need to obtain information that is strictly necessary to provide a service to an individual; whether there is a less restrictive method of obtaining personal information that will enable the State to provide the same service; what is the nature of the service for which the <em>use</em> of the Digital ID is being made mandatory; prevention of harm to others, and eligibility to undertake specialised tasks.</p>

							<p class="indent">It is important to note the distinction between foundational and functional identity systems here. A foundational identity system is a core Identity System created to manage identity information for the general public, and to provide identity proof for a wide variety of public and private services. On the other hand, a functional identity system is designed to meet the needs of an individual sector, and is not designed for, though, in some cases, may be used for other purposes or in other sectors. Within specific sectors, it is much more acceptable to mandate the use of functional identity systems which pertain to them.</p>
						</div>
					</section>

				<!-- Divider -->
				<div id="divider"><hr /></div>

				<!-- Section -->
					<section>
						<header>
							<h3>Risk based Tests</h3>
						</header>
						<div class="content">
							<p>The debate and discussion around Digital ID has centered primarily on the perceived or existing risks related to privacy, welfare, equality and inclusion. As a range of use cases of Digital ID emerge, laws and institutions governing Digital ID must be vigilant about the risks and harms emerging from them. This needs to be done with some urgency regarding the existing use cases of Digital ID, as well. A rights based approach is, by itself, not sufficient to address these challenges, and there is a need for greater paternalistic regulatory measures that strictly govern the nature of uses of Digital ID. Below we attempt to articulate some draft principles. These principles do not exist in most jurisdiction dealing with Digital ID, though there is now an increasing focus on harms assessment in prominent frameworks such as the GDPR.</p>
						</div>
					</section>

				<!-- Section -->
					<section>
						<header>
							<h3>3.1</h3>
						</header>
						<div class="content">
							<h5>Risk Assessment</h5>
							<h4 style="margin-bottom: 0.75em; font-size: 1.5em; line-height: 1.2em;">Are decisions regarding the legitimacy of uses, benefits of using Digital ID, and their impact on individual rights informed by risk assessment?</h4>
							
							<p>Borrowing from principles of law that seek to protect consumers, laws governing Digital ID need to take into account tangible harms to individuals, have clear provisions on prevention and appropriate recovery for those harms, if they occur.</p>

							<p class="indent">Digital IDs combine technology, big data processing abilities, with vast quantities of biographical data, that carry risks of profiling, surveillance, and chilling effects.<sup class="superscript"><a href="#fn81">81</a></sup><a name="ref81"></a> Other risks associated with Digital IDs include human execution errors, unauthorized use, exclusion of individuals, and surveillance.<sup class="superscript"><a href="#fn82">82</a></sup><a name="ref82"></a></p>

							<p class="indent">A risk-based approach to privacy requires that the digital ID system not be exclusively examined against constitutional rights guaranteed to individuals, but also against actual/potential tangible harms they may suffer. A risk-based approach to privacy does not act as a ‘substitute for legal compliance’; instead, it helps the government/administrator identify the risks, prioritise them in an order of severity and probability; and act accordingly.<sup class="superscript"><a href="#fn83">83</a></sup><a name="ref83"></a> A risk-based approach thus allows the administrator to take the full benefit of the Digital ID, while being cognizant of, and protecting, the rights of the citizens. </p>

							<p class="indent">Risk assessment requires identifying a ‘privacy risk,’ i.e. a feared event, and the ‘risk sources,’ i.e. the manner in which such a feared event can be reached and avoided.<sup class="superscript"><a href="#fn84">84</a></sup><a name="ref84"></a> In Digital IDs, these risks can be classified into various forms, such as —</p>
						</div>
					</section>

					<section>
						<header>
							<h4>(i)</h4>
						</header>
						<div class="content">
							<h4>Privacy Harms</h4><br/>
							<p>The invasion of the right to privacy through the unauthorised transfer of personal data or through mission creep; for instance, a foreseeable risk of linking a digital ID with individuals’ financial data, is that of the data being used to inform credit scoring without their consent or knowledge. The risk sources associated with this will depend on the amount of financial data accessible through the Digital ID; the role of private (finance) companies; the security and infrastructure of the system, etc.</p>
						</div>
					</section>

					<section>
						<header>
							<h4>(ii)</h4>
						</header>
						<div class="content">
							<h4>Exclusion Harms</h4><br/>
							<p>The denial of benefits that are linked to authentication of the Digital ID; for instance, denial of access to (essential) services due to reliance on imperfect biometric authentication, or the use of online authentication where the availability of internet is poor.</p>
						</div>
					</section>

					<section>
						<header>
							<h4>(iii)</h4>
						</header>
						<div class="content">
							<h4>Discriminatory Harms</h4><br/>
							<p>The misuse of the date collected, generated, and stored to help profile certain individuals and target them for their political views. Discriminatory harms can also be connected to exclusion harms in cases of identification via fingerprints or other electronic means, since these are disproportionately harder for those belonging to the marginalised class.</p>
							<br/><br/>
							
							<p>Risk assessment is aided by the enactment of a data protection law before the national Digital ID law, since the former lays down the governing standards for data collection, retention, storage, use, and sharing policies, and also sets up a proper civil and criminal enforcement framework.</p>
						</div>
					</section>

				<!-- Section -->
					<section>
						<header>
							<h3>3.2</h3>
						</header>
						<div class="content">
							<h5>Differentiated Approaches to Risk</h5>
							<h4>Do the laws and regulations envisage a differentiated approach to governing uses of Digital ID, based on the risks it entails?</h4>

							<p>Implicit in a risks based assessment is governance that is specific to the nature of harm it is attempting to address. Drawing from Fred Cate’s model of harms in data protection,<sup class="superscript"><a href="#fn85">85</a></sup><a name="ref85"></a> a differentiated approach may involve categorising uses as —</p>

						</div>
					</section>

					<section>
						<header>
							<h4>(i)</h4>
						</header>
						<div class="content">
							<h4><em>Per Se</em> Harmful</h4><br/>
							<p>Where a use is always harmful, such as when the use of ID to collect and use alternative data proven to be predatory for credit scoring and lending or the use of personal information to commit fraud, the regulator could prohibit the use outright.</p>
						</div>
					</section>

					<section>
						<header>
							<h4>(ii)</h4>
						</header>
						<div class="content">
							<h4><em>Per Se</em> Not Harmful</h4><br/>
							<p>The regulator may consider not regulating uses that present no reasonable likelihood of harm, such as where Digital ID is used simply as one of many forms of ID for identification purposes (such as passports and driving licenses).</p>
						</div>
					</section>

					<section>
						<header>
							<h4>(iii)</h4>
						</header>
						<div class="content">
							<h4>Sensitive Uses</h4><br/>
							<p>Where use of personal data is neither “per se harmful” nor “per se not harmful,” the regulator may condition the use on several factors, such as aligning with a rights based approach and requiring the consent of the user. For instance, the use of Digital ID for identification via authorisation or authentication can lead to exclusion, and hence, the government should always provide for offline or alternative verification mechanisms so that no user is denied the benefits that are linked to the identification.</p>
						</div>
					</section>

				<!-- Section -->
					<section>
						<header>
							<h3>3.3</h3>
						</header>
						<div class="content">
							<h5>Proportionality</h5>
							<h4 style="margin-bottom: 0.75em; font-size: 1.5em; line-height: 1.2em;">Does the law on Digital ID envisage governance, which is proportional to the likelihood and severity of the possible risks of its use?</h4>

							<p>Regulation of Digital ID needs to be sensitive to the actual harms caused by its uses, and be informed by the severity and likelihood of the harm. The risk level is estimated in terms of severity and likelihood. Severity represents the magnitude of a risk. It essentially depends on the consequences of the potential impacts. Likelihood represents the probability of a risk to occur. It depends on the level of vulnerabilities of the supporting assets facing the level of capabilities of the risk sources to exploit them.<sup class="superscript"><a href="#fn86">86</a></sup><a name="ref86"></a></p>

							<p class="indent">Risk assessment requires that the harms be clearly identified and graded on a scale of severity. After this, the risk factors for each of these harns should be delineated and graded based on the likelihood of occurrence. These diverse risks in the Digital ID system then have to be addressed in the law commensurately, using proportionate measures. For instance, a design involving the centralised storage of biometric data with robust security safeguards may have a remote likelihood of security risk, but have a very high severity in cases of breach. Biometrics, being permanent and unchangeable, pose a great risk of identity theft if they are stolen. Therefore, it is imperative for the government to seriously consider whether, as part of ‘privacy by design’, the Digital ID needs to be designed this way. A proportionality analysis at this stage would suggest using less restrictive measures – i.e. measures that reduce the likelihood of harm – to prevent ID theft, if the government does decide to go ahead with centralised databases (thus, keeping the severity of harm constant).</p>

							<p class="indent">The risk of identity theft can be mitigated by not collecting permanent and irreversible biometric information, which once compromised, cannot be changed (unlike a credit card password). Even if biometric data were to be collected, it would be preferable if on-device authentication is conducted by using the biometric data as a ‘password’, instead of employing centralised cloud storage and authentication.<sup class="superscript"><a href="#fn87">87</a></sup><a name="ref87"></a> It would also be more secure — from a cyber security perspective<sup class="superscript"><a href="#fn88">88</a></sup><a name="ref88"></a> — to separate the agencies overseeing the collection/identification task with the authentication task. The law should also set up a strong grievance redress and enforcement mechanism. Finally, prompt action should be taken by the government in the face of a security risk.</p>

							<p class="indent">While risks associated with a digital ID system might depend on the design and architecture of the system, the threats a system is susceptible to can be analysed based on its use. Threats can be indicated by the number of entities/actors attempting to breach the system, which would in turn depend on the incentives available to them.<sup class="superscript"><a href="#fn89">89</a></sup><a name="ref89"></a> The wider the scope of services associated with the digital ID, the greater the threats it faces, thus increasing the overall risk of the system. For instance, where the digital ID system is connected to financial data of individuals, incentives for breach range from theft to collecting data for credit reporting companies; where the digital ID is connected to individuals’ mobile numbers, it becomes possible to trace their online activities, thus subjecting the system to political motives. A system that contains many vulnerabilities but has limited threats, because of the narrow scope of its service, would present as overall low risk.</p>

							<p class="indent">In this way, the possible uses of a system form a large part of the risk assessment of the entire system, and must be proportionally factored into the regulatory structure. This could also take the form of requiring different identifiers or having different security measures based on a particular use of the system, where the system has multiple uses, with the intention of reducing the overall risk in the most cost and operational efficient manner.</p>

							<p class="indent">Another example of a proportionality analysis using the risk assessment model deals with exclusion. While conducting a risk assessment, the authorities will have to take into account the nature of personal information being collected. It is well-recognised that while dealing with biometrics, ‘human recognition systems are inherently probabilistic, and hence inherently fallible. The chance of error can be made small but not eliminated,’&thinsp;<sup class="superscript"><a href="#fn90">90</a></sup><a name="ref90"></a> making exclusion errors for some individuals an <em>inevitable</em> reality. These harms have to be clearly recognised in the law, which should incorporate preventive and remedial measures in the law itself. In the present case, for instance, the risk of exclusion can be reduced by making use of the Digital ID voluntary.</p>

							<p class="indent">Some of the risks<sup class="superscript"><a href="#fn91">91</a></sup><a name="ref91"></a> inherent in a Digital ID system and possible governance solutions are listed here —</p>

						</div>
					</section>

					<section>
						<header>
							<h4>(i)</h4>
						</header>
						<div class="content">
							<h4>Inaccurate Data Collection</h4>
							<p>This can lead to exclusion while accessing benefits. It is important for the law to provide the right to access and correction, so that any incorrect data can be quickly rectified.</p>
						</div>
					</section>

					<section>
						<header>
							<h4>(ii)</h4>
						</header>
						<div class="content">
							<h4>Authentication Errors</h4>
							<p>Problems with processing of ID credentials can result in false positives or false negatives during authentication. The law should connect the identity of the individual to a unique number, and not limit it to a biometric identifier.<sup class="superscript"><a href="#fn92">92</a></sup><a name="ref92"></a> It may also help to improve data quality,  the quality of the device capturing biometrics, and (ironically) collect more data, since a more expansive database may reduce the risk of false positives.<sup class="superscript"><a href="#fn93">93</a></sup><a name="ref93"></a> Allowing alternate means of authentication where possible would help mitigate the harms of failure.</p>
						</div>
					</section>

					<section>
						<header>
							<h4>(iii)</h4>
						</header>
						<div class="content">
							<h4>Mission Creep</h4>
							<p>The use of collected data for an extraneous purpose, without the user’s consent, results in mission creep or an unauthorised use of data. Given the possible risks of this use, the Digital ID law should clearly incorporate the principle of purpose limitation, and require that each new use of data require a fresh consent. The law should also stipulate a wide range of civil and criminal penalties for various acts, and should have a strong grievance redress and enforcement mechanism. Besides having criminal or civil penalties, there must also be a strong oversight mechanism/body to identify when such harms are occurring, and means to address the harm itself, including mandatory deletion of data etc.</p>
						</div>
					</section>

					<section>
						<header>
							<h4>(iv)</h4>
						</header>
						<div class="content">
							<h4>Indiscriminate Data Sharing</h4>
							<p>The potential of indiscriminate sharing or transfer of data between the regulator of the Digital ID, various State and private agencies, and third parties, poses significant privacy risks. Again, the law should criminalise such actions, and have a strong enforcement system. For instance, allowing citizens to access information about where their data is going, implementing a mechanism to always record flow of data, with time logs etc, are best practises to prevent unauthorised data sharing.</p>
						</div>
					</section>

					<section>
						<header>
							<h4>(v)</h4>
						</header>
						<div class="content">
							<h4>Identity Theft</h4>
							<p>This remains a key risk of any identification scheme, if not done properly. Even though the Digital ID has very secure systems of collection and storage, even one instance of data theft, howsoever remote the possibility, can have potentially irreversible consequences (especially if biometrics are compromised). Hence, it is very important to prepare for such an eventuality – by implementing access control, limiting retention periods, criminalising such actions etc. Having a national data protection law and judicial oversight mechanisms already in place is also necessary to instil good data collection and storage practices and have a strong <em>working</em> enforcement system.</p>
						</div>
					</section>

				<!-- Section -->
					<section>
						<header>
							<h3>3.4</h3>
						</header>
						<div class="content">
							<h5>Response to Risks</h5>
							<h4 style="margin-bottom: 0.75em; font-size: 1.5em; line-height: 1.2em;">In cases of demonstrable high risk from uses of Digital ID, are there mechanisms in place to prohibit or restrict the use?Do the laws and regulations envisage a differentiated approach to governing uses of Digital ID, based on the likelihood and severity of risk?</h4>

							<p>Responding to risks that are inevitable in a Digital ID system are crucial to the functioning of the system. The World Bank recommends a two pronged approach to mitigating risks — first, conducting an ‘ID Enabling Environment Assessment (IDEEA)’ to study a country’s legal and regulatory data privacy framework and highlight areas of improvement; and second, incentivising Privacy by Design features within the Digital ID law itself.<sup class="superscript"><a href="#fn94">94</a></sup><a name="ref94"></a></p>

							<p class="indent">For instance, the ID enabling environment can mitigate risks by making the Digital ID just one of the multiple choices and identities by which citizens can identify themselves. This is because a data breach or a security shortfall will not compromise the <em>only</em> ID that people have. For instance, if the national ID is the <em>only</em> accepted ID for operating a bank account, getting a SIM connection, and receiving benefits from the State; then any single security failure will compromise the entire security database.<sup class="superscript"><a href="#fn95">95</a></sup><a name="ref95"></a></p>

							<p class="indent">A Privacy by Design approach can be adopted by using derived/temporary identification numbers that mask the actual Digital ID number, which means that even in case of a data breach, the actual ID is not exposed.<sup class="superscript"><a href="#fn96">96</a></sup><a name="ref96"></a> For instance, Austria’s national ID system has used ‘tokenization’ as a privacy by design principle.<sup class="superscript"><a href="#fn97">97</a></sup><a name="ref97"></a> The Indian government is also attempting to use Virtual IDs and tokens in place of the Aadhaar number.</p>

							<p class="indent">Among other factors that must be considered while responding to a risk is that of the nature of the risk/harm. When a breach or failure of the ID system occurs in some manner, the effects can be either reversible or irreversible, notwithstanding the severity of the harm. For instance, if financial information of an ID holder has been accessed, while the harm is significant, it can be addressed by notifying the financial institutions, invalidating the ID credentials, etc. On the other hand, where personal information about an ID holder has been used to profile them, or to influence their choices like in the instance of the Cambridge Analytica data breach, the effects cannot easily be reversed, even by subsequently closing off the breach. Thus, even where the severity of the breach itself may not be very significant, the irreversible nature of the harm warrants attention.</p>

							<p class="indent">For the risks that continue to remain in Digital ID, due to the benefits that the system introduces to society, a mitigation strategy must be employed. This could involve a duty to notify ID holders of any breach to their data; establishing a body to prevent and investigate cyberthreats; having in place a (tested) business continuity plan to regain regular operations; investing in capacity building, etc. This risk mitigation strategy would also depend on the model and design of the digital ID system, and its reliance on private companies to provide IDs, authentication services, collect information etc.</p>

							<p class="indent">If the risks from uses of Digital ID are demonstrably high, they need to be restricted until there are adequate mitigating factors that can be introduced. This may need a responsive Digital ID regulator, who has the mandate and resources to intervene responsively. In Estonia, for example, when a security flaw in around 750,000 national Digital ID cards came to light in 2017, making the ID cards susceptible to identity theft, the government took immediate preventive action and declared that the security certificates of the ID cards would be disabled.<sup class="superscript"><a href="#fn98">98</a></sup><a name="ref98"></a> They eventually laid out the lessons learnt from managing the risks.<sup class="superscript"><a href="#fn99">99</a></sup><a name="ref99"></a></p>
						</div>
					</section>

				<!-- Divider -->
				<div id="divider"><hr style="border-width: 3px;"/></div>


					<section>

						<div class="content">
							<h3>Notes</h3>
							<br/>

							<table class="footnote">
  								<tr>
    								<td class="number">1</td>
    								<td class="reference"><a name="fn1"></a>This is the legality prong of the proportionality tests used in most common law jurisdictions.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref1">&uarr;</a></span></td>
  								</tr>
  								<tr>
    								<td class="number">2</td>
    								<td class="reference"><a name="fn2"></a><em>US v.  Jones</em>, 132 S. Ct. 945, 956 (2012) (Sotomayor, J., concurring) noting that <em>“Awareness that the Government may be watching chills associational and expressive freedoms.”</em>; Niva Elkin-Koren, Michal S. Gal, “The Chilling Effect of Governance-by-Data on Data Markets”, 86 <em>University of Chicago Law Review</em> (2019): 403.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref2">&uarr;</em></a></span></td>
  								</tr>
  								<tr>
    								<td class="number">3</td>
    								<td class="reference"><a name="fn3"></a><em>State of Madhya Pradesh v. Thakur Bharat Singh</em>, 2 SCR 454 (1967).&nbsp;&nbsp;<span class="internal-nav"><a href="#ref3">&uarr;</em></a></span></td>
  								</tr>
  								<tr>
    								<td class="number">4</td>
    								<td class="reference"><a name="fn4"></a><em>Kharak Singh v. State of U.P.</em>, 1 SCR 332 (1964); <em>Bijoe Emmanuel v. State of Kerala</em>, 3 SCC 615 (1986), at paras 16, 19.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref4">&uarr;</em></a></span></td>
  								</tr>
  								<tr>
    								<td class="number">5</td>
    								<td class="reference"><a name="fn5"></a><em>K.S. Puttaswamy v. Union of India</em>, 10 SCC 1 (2017), paras 310, 325 (Chandrachud J.), 638 (Kaul J.) [<em>“Puttaswamy”</em>]; <em>K.S. Puttaswamy v Union of India (II)</em>, 1 SCC 1 (2019), at paras 147, 557 [<em>“Aadhaar Judgment”</em>].&nbsp;&nbsp;<span class="internal-nav"><a href="#ref5">&uarr;</em></a></span></td>
  								</tr>
  								<tr>
    								<td class="number">6</td>
    								<td class="reference"><a name="fn6"></a><em>Provincial Picture Houses v. Wednesbury Corporation</em>, (1947) 1 KB 223.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref6">&uarr;</em></a></span></td>
  								</tr>
  								<tr>
    								<td class="number">7</td>
    								<td class="reference"><a name="fn7"></a><em>R v. Big M Drug Mart Ltd.</em>, (1985) 1 SCR 295.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref7">&uarr;</em></a></span></td>
  								</tr>
  								<tr>
    								<td class="number">8</td>
    								<td class="reference"><a name="fn8"></a><em>S v. Makwanyane</em>, (1995) 3 SA 391 (CC).&nbsp;&nbsp;<span class="internal-nav"><a href="#ref8">&uarr;</em></a></span></td>
  								</tr>
  								<tr>
    								<td class="number">9</td>
    								<td class="reference"><a name="fn9"></a><em>Belvedere Alberghiera v. Italy</em>, 31524/96, (2000), at 56-58.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref9">&uarr;</em></a></span></td>
  								</tr>
  								<tr>
    								<td class="number">10</td>
    								<td class="reference"><a name="fn10"></a><em>Uzun v. Germany</em>, 53 EHRR 852 (2010), para 60; Perry v. UK, 39 EHRR 3, (2004) para 45.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref10">&uarr;</em></a></span></td>
  								</tr>
  								<tr>
    								<td class="number">11</td>
    								<td class="reference"><a name="fn11"></a><em>Malone v. UK</em>, ECHR 10, (1984) at para 67.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref11">&uarr;</em></a></span></td>
  								</tr>
  								<tr>
    								<td class="number">12</td>
    								<td class="reference"><a name="fn12"></a><em>Slivenko v. Latvia</em>, 48321/99 (2003).&nbsp;&nbsp;<span class="internal-nav"><a href="#ref12">&uarr;</a></span></td></td>
  								</tr>
  								<tr>
    								<td class="number">13</td>
    								<td class="reference"><a name="fn13"></a><em>Malone v. UK</em>, ECHR 10, (1984).&nbsp;&nbsp;<span class="internal-nav"><a href="#ref13">&uarr;</a></span></td></td>
  								</tr>
  								<tr>
    								<td class="number">14</td>
    								<td class="reference"><a name="fn14"></a><em>Vukota-Bojić v. Switzerland</em>, ECHR 899, (2016) at paras 73, 77; <em>Piechowicz v. Poland</em>, ECHR 689,(2012) at para 212.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref14">&uarr;</a></span></td></td>
  								</tr>
  								<tr>
    								<td class="number">15</td>
    								<td class="reference"><a name="fn15"></a>Victoria Aitken, “An Exposition Of Legislative Quality And Its Relevance For Effective Development,” 2 <em>PROLAW Student Journal</em>, 1-43 (2013); Helen Xanthaki, <em>Drafting Legislation: Art and Technology of Rules for Regulation</em> (Hart Publishing, 2014); Vrinda Bhandari and Renuka Sane, “A Critique of the Aadhaar Legal Framework,” 31(1) NLSIR 1 (2019) (forthcoming).&nbsp;&nbsp;<span class="internal-nav"><a href="#ref15">&uarr;</em></a></span></td>
  								</tr>
  								<tr>
    								<td class="number">16</td>
    								<td class="reference"><a name="fn16"></a>Timothy Endicott, <em>Vagueness in Law</em> (OUP, 2000).&nbsp;&nbsp;<span class="internal-nav"><a href="#ref16">&uarr;</em></a></span></td>
  								</tr>
  								<tr>
    								<td class="number">17</td>
    								<td class="reference"><a name="fn17"></a><em>Grayned v. City of Rockford</em>, 408 U.S. 104, 108 (1972); <em>A.N. Parasuraman v. State of Tamil Nadu</em>, 4 SCC 683 (1989).&nbsp;&nbsp;<span class="internal-nav"><a href="#ref17">&uarr;</em></a></span></td>
  								</tr>
  								<tr>
    								<td class="number">18</td>
    								<td class="reference"><a name="fn18"></a><em>Peruzzo & Martens v. Germany</em>, ECHR 743, (2013), para 34.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref18">&uarr;</em></a></span></td>
  								</tr>
  								<tr>
    								<td class="number">19</td>
    								<td class="reference"><a name="fn19"></a><em>S & Marper v. UK</em>, ECHR 1581, (2008), para 101.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref19">&uarr;</em></a></span></td>
  								</tr>
  								<tr>
    								<td class="number">20</td>
    								<td class="reference"><a name="fn20"></a>For an analysis of ECtHR’s jurisprudence on this point, see, Steven Greer, “The exceptions to Articles 8 to 11 of the European Convention on Human Rights”, Human Rights File No. 15, Council of Europe (1997), <a href="https://www.echr.coe.int/LibraryDocs/DG2/HRFILES/DG2-EN-HRFILES-15(1997).pdf" target="_blank"><em>https://www.echr.coe.int/LibraryDocs/DG2/HRFILES/DG2-EN-HRFILES-15(1997).pdf</em></a>, at 14.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref20">&uarr;</em></a></span></td>
  								</tr>
  								<tr>
    								<td class="number">21</td>
    								<td class="reference"><a name="fn21"></a>Article 8(2), European Convention of Human Rights, 1950; <em>Puttaswamy</em>, supra, paras 311-312.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref21">&uarr;</em></a></span></td>
  								</tr>
  								<tr>
    								<td class="number">22</td>
    								<td class="reference"><a name="fn22"></a><em>Mozer v. the Republic of Moldova and Russia</em>, No. 11138/2010, (Grand Chamber) (2016), para 194.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref22">&uarr;</em></a></span></td>
  								</tr>
  								<tr>
    								<td class="number">23</td>
    								<td class="reference"><a name="fn23"></a>Principle 2, Necessity and Proportionality Principles, International Principles on the Application of Human Rights to Communication Surveillance (May, 2014) available at <a href="https://necessaryandproportionate.org/principles#principle2" target="_blank"><em>https://necessaryandproportionate.org/principles#principle2</em></a>.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref23">&uarr;</em></a></span></td>
  								</tr>
  								<tr>
    								<td class="number">24</td>
    								<td class="reference"><a name="fn24"></a>Daniel Solove, “10 Reasons Why Privacy Matters”, <em>TeachPrivacy (blog)</em>, January 20 2014, <a href="https://teachprivacy.com/10-reasons-privacy-matters/" target="_blank"><em>https://teachprivacy.com/10-reasons-privacy-matters/</em></a>.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref24">&uarr;</em></a></span></td>
  								</tr>
  								<tr>
    								<td class="number">25</td>
    								<td class="reference"><a name="fn25"></a>1 SCC 1 (2019).&nbsp;&nbsp;<span class="internal-nav"><a href="#ref25">&uarr;</em></a></span></td>
  								</tr>
  								<tr>
    								<td class="number">26</td>
    								<td class="reference"><a name="fn26"></a><em>Aadhaar Judgment, supra</em>, para (447)(4)(h). For a further analysis on the Court’s reasons for striking down Section 57 of the Aadhaar Act, see Vrinda Bhandari and Rahul Narayan, “In striking down Section 57, SC has curtailed the function creep and financial future of Aadhaar,” <em>The Wire</em>, Sept. 28, 2018, <a href="https://thewire.in/law/in-striking-down-section-57-sc-has-curtailed-the-function-creep-and-financial-future-of-aadhaar" target="_blank"><em>https://thewire.in/law/in-striking-down-section-57-sc-has-curtailed-the-function-creep-and-financial-future-of-aadhaar</em></a>. &nbsp;&nbsp;<span class="internal-nav"><a href="#ref26">&uarr;</em></a></span></td>
  								</tr>
  								<tr>
    								<td class="number">27</td>
    								<td class="reference"><a name="fn27"></a>Through an amendment and ordinance, the government has now sought to (re-)introduce voluntary private sector involvement in the Aadhaar and Other Laws (Amendment) Bill, 2019, with diverging views on whether this is legal and/or appropriate. For arguments on why the Amendment is contrary to the Judgment of the Supreme Court see Raghu, “Six Reasons Why the Aadhaar Amendment Ordinance Undermines Democracy,” <em>The Wire</em>, Mar. 12, 2019, <a href="https://thewire.in/government/aadhaar-amendments-ordinance-democracy" target="_blank"><em>https://thewire.in/government/aadhaar-amendments-ordinance-democracy</em></a>; Vrinda Bhandari, “Why Amend the Aadhaar Act Without First Passing a Data Protection Bill?”, <em>The Wire</em>, Jan. 4, 2019, <a href="https://thewire.in/law/aadhaar-act-amendment-data-protection" target="_blank"><em>https://thewire.in/law/aadhaar-act-amendment-data-protection</em></a>. For (opposing) arguments endorsing the government’s amendment, see Rahul Matthan, “The Aadhaar Amendment and Private Sector Access”, <em>LiveMint</em>, 08 Jan 2019, <a href="https://www.livemint.com/Opinion/jmxPkXXGWeEfiAsCsA1xnO/Opinion--The-Aadhaar-amendment-and-private-sector-access.html" target="_blank"><em>https://www.livemint.com/Opinion/jmxPkXXGWeEfiAsCsA1xnO/Opinion--The-Aadhaar-amendment-and-private-sector-access.html</em></a>; Nehaa Chaudhari, “Supreme Court has banned private companies from using Aadhaar. What does it actually mean?”, <em>Scroll.in</em>, October 4 2018, <a href="https://scroll.in/article/896771/supreme-court-has-banned-private-companies-from-using-aadhaar-what-does-it-actually-mean." target="_blank"><em>https://scroll.in/article/896771/supreme-court-has-banned-private-companies-from-using-aadhaar-what-does-it-actually-mean.</em></a>&nbsp;&nbsp;<span class="internal-nav"><a href="#ref27">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">28</td>
    								<td class="reference"><a name="fn28"></a>“State Laws Restricting Private Use of Social Security Numbers.” Advocacy. Accessed August 21, 2019. <a href="https://advocacy.consumerreports.org/press_release/state-laws-restricting-private-use-of-social-security-numbers/" target="_blank"><em>https://advocacy.consumerreports.org/press_release/state-laws-restricting-private-use-of-social-security-numbers/</em></a>.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref28">&uarr;</em></a></span></td>
								</tr>
								<tr>
    								<td class="number">29</td>
    								<td class="reference"><a name="fn29"></a>Access Now, “National Digital Identity Programmes: What’s Next?”, March 2018, 22 <a href="https://www.accessnow.org/cms/assets/uploads/2018/03/Digital-Identity-Paper-digital-version-Mar20.pdf" target="_blank"><em>https://www.accessnow.org/cms/assets/uploads/2018/03/Digital-Identity-Paper-digital-version-Mar20.pdf</em></a>. [“AccessNow”]&nbsp;&nbsp;<span class="internal-nav"><a href="#ref29">&uarr;</em></a></span></td>
								</tr>
								<tr>
    								<td class="number">30</td>
    								<td class="reference"><a name="fn30"></a>Principles 3 and 5, Necessity and Proportionality Principles, <em>International Principles on the Application of Human Rights to Communication Surveillance</em> (May, 2014), <a href="https://necessaryandproportionate.org/principles#principle3" target="_blank"><em>https://necessaryandproportionate.org/principles#principle3</em></a>. &nbsp;&nbsp;<span class="internal-nav"><a href="#ref30">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">31</td>
    								<td class="reference"><a name="fn31"></a><em>Puttaswamy, supra</em>, para 311.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref31">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">32</td>
    								<td class="reference"><a name="fn32"></a><em>Puttaswamy, supra</em>.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref32">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">33</td>
    								<td class="reference"><a name="fn33"></a>AccessNow, <em>supra</em>, 23, 25.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref33">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">34</td>
    								<td class="reference"><a name="fn34"></a>For instance, Section 15 of the Indian Census Act, the records of the census are not admissible as evidence in any civil or criminal proceedings. See also the ‘Purpose Specification’ Principle and the ‘Use Limitation’ Principle in the OECD Privacy Framework, <a href="http://www.oecd.org/sti/ieconomy/oecd_privacy_framework.pdf" target="_blank"><em>http://www.oecd.org/sti/ieconomy/oecd_privacy_framework.pdf</em></a>, at 14.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref34">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">35</td>
    								<td class="reference"><a name="fn35"></a><em>S & Marper, supra</em>, para 103.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref35">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">36</td>
    								<td class="reference"><a name="fn36"></a>Nikhil Pahwa, “Learning from Aadhaar: 10 rules from nations on how not to make a mess of their digital IDs,” <em>Scroll.in</em>, <a href="https://scroll.in/article/858767/learning-from-aadhaar-10-rules-for-nations-on-how-not-to-make-a-mess-of-their-national-ids" target="_blank"><em>https://scroll.in/article/858767/learning-from-aadhaar-10-rules-for-nations-on-how-not-to-make-a-mess-of-their-national-ids</em></a> [“Nikhil Pahwa”].&nbsp;&nbsp;<span class="internal-nav"><a href="#ref36">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">37</td>
    								<td class="reference"><a name="fn37"></a>Report of the Group of Experts on Privacy, Government of India Planning Commission (October 16, 2012), <a href="https://www.dsci.in/content/report-group-experts-privacyconstituted-planning-commission-india" target="_blank"><em>https://www.dsci.in/content/report-group-experts-privacyconstituted-planning-commission-india</em></a>, [“Justice AP Shah Report”] 22, 70; AccessNow, <em>supra</em>, 29.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref37">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">38</td>
    								<td class="reference"><a name="fn38"></a><em>Indian Soaps & Toiletries Makers Association v. Ozair Hussain</em>, 3 SCC 641, (2013), at paras 28 and 29; <em>Reliance Petrochemicals v Indian Express</em>, 4 SCC 592, (1988) para 34.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref38">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">39</td>
    								<td class="reference"><a name="fn39"></a>Justice AP Shah Report, <em>supra</em>, 25.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref39">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">40</td>
    								<td class="reference"><a name="fn40"></a>See in the context of general data protection legislation, Sections 45 (right of access) and 46 (right of rectification) of the UK Data Protection Act, 1998.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref40">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">41</td>
    								<td class="reference"><a name="fn41"></a>For elements of a well-designed grievance redress framework, see Vrinda Bhandari and Renuka Sane, <em>Critique of the Aadhaar Legal Framework, supra</em>, 18-20.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref41">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">42</td>
    								<td class="reference"><a name="fn42"></a><em>Sahara India v CIT</em>, 14 SCC 151, (2008) para 29.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref42">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">43</td>
    								<td class="reference"><a name="fn43"></a>The Indian Aadhaar experience is instructive to understand the issues with criminal redress. Despite designating various actions as specific criminal offences, section 47 of the Aadhaar Act permitted only the UIDAI to initiate criminal prosecution, thereby eliminating the involvement of the Aadhaar number holder entirely. The constitutionality of this provision was challenged before the Supreme Court, which held <em>“Insofar as Section 47 of the Act which provides for the cognizance of offence only on a complaint made by the Authority or any officer or person authorised by it is concerned, it needs a suitable amendment to include the provision for filing of such a complaint by an individual/victim as well whose right is violated.”</em> Subsequently, the government added a proviso to Section 47 through the Aadhaar Amendment Act, 2019, allowing the Aadhaar number holder or individual to file a criminal complaint for the commission of certain specified offences.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref43">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">44</td>
    								<td class="reference"><a name="fn44"></a>Privacy by Design: Current Practises in Estonia, India, and Austria, World Bank Group, 2018, <a href="https://id4d.worldbank.org/sites/id4d.worldbank.org/files/PrivacyByDesign_112918web.pdf" target="_blank"><em>https://id4d.worldbank.org/sites/id4d.worldbank.org/files/PrivacyByDesign_112918web.pdf</em></a>, [“ID4D WB”] 5.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref44">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">45</td>
    								<td class="reference"><a name="fn45"></a>OECD Principles, <em>supra</em>; Justice Shah Report, <em>supra</em>, 27.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref45">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">46</td>
    								<td class="reference"><a name="fn46"></a>Accountability: A Compendium for Stakeholders, The Centre for Information Policy Leadership, 2011, <a href="http://informationaccountability.org/wp-content/uploads/Centre-Accountability-Compendium.pdf" target="_blank"><em>http://informationaccountability.org/wp-content/uploads/Centre-Accountability-Compendium.pdf</em></a>, at 3.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref46">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">47</td>
    								<td class="reference"><a name="fn47"></a>For a detailed discussion on ex-ante and ex-post accountability, see Vrinda Bhandari and Renuka Sane, NLSIR, <em>supra</em>, 13.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref47">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">48</td>
    								<td class="reference"><a name="fn48"></a>AccessNow, <em>supra</em>, 25.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref48">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">49</td>
    								<td class="reference"><a name="fn49"></a>See the observations made by Chandachud J. in his dissent in <em>Aadhaar Judgment</em>, para 1539.5 (Chandrachud J). See also NLSIR, <em>supra</em>.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref49">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">50</td>
    								<td class="reference"><a name="fn50"></a>T.H.A. Wisman, "Purpose and function creep by design: Transforming the face of surveillance through the Internet of Things", <em>European Journal of Law and Technology</em>, Vol. 4, No. 2 (2013), <a href="http://ejlt.org/article/view/192/379#_ftnref11" target="_blank"><em>http://ejlt.org/article/view/192/379#_ftnref11</em></a>; M. Granger Morgan and Elaine Newton, “Protecting Public Anonymity”, <em>21 Issues in Science and Technology</em> (2004).&nbsp;&nbsp;<span class="internal-nav"><a href="#ref50">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">51</td>
    								<td class="reference"><a name="fn51"></a>Nancy YueLiu, <em>Bio-Privacy: Privacy Regulation and the Challenge of Biometrics</em> (Routledge, 2012), 72-73 [“Liu”].&nbsp;&nbsp;<span class="internal-nav"><a href="#ref51">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">52</td>
    								<td class="reference"><a name="fn52"></a><em>Vinod Kumar v. Ashok Kumar Gandhi</em>, 1 SCC 1, (2019) para 1357.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref52">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">53</td>
    								<td class="reference"><a name="fn53"></a><em>Leander v Sweden</em>, ECHR 4, (1987) para 48; <em>MK v France</em>, ECHR 341 (2013); <em>S & Marper v. UK</em>, ECHR 1581, (2008), para 67.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref53">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">54</td>
    								<td class="reference"><a name="fn54"></a><em>Amann v. Switzerland</em>, ECHR 88 (2000), para 69.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref54">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">55</td>
    								<td class="reference"><a name="fn55"></a><em>Puttaswamy, supra</em>, paras 311, 328 (Chandrachud J.), 640 (Kaul J.)&nbsp;&nbsp;<span class="internal-nav"><a href="#ref55">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">56</td>
    								<td class="reference"><a name="fn56"></a><em>S & Marper, supra</em>, para 101; <em>MK v France, supra</em>, para 33.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref56">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">57</td>
    								<td class="reference"><a name="fn57"></a>Steven Greer. <em>The exceptions to Articles 8 to 11 of the European Convention on Human Rights.</em> Vol. 88. Council of Europe, 1997, <a href="https://www.echr.coe.int/LibraryDocs/DG2/HRFILES/DG2-EN-HRFILES-15(1997).pdf" target="_blank"><em>https://www.echr.coe.int/LibraryDocs/DG2/HRFILES/DG2-EN-HRFILES-15(1997).pdf</em></a>, at 15.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref57">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">58</td>
    								<td class="reference"><a name="fn58"></a><em>MK v France, supra</em>, para 40.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref58">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">59</td>
    								<td class="reference"><a name="fn59"></a><em>Puttaswamy, supra</em>, paras 310, 325, 638, 639.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref59">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">60</td>
    								<td class="reference"><a name="fn60"></a><em>Aadhaar Judgment, supra</em>, paras 319, 494, 511.5 (Sikri J.)&nbsp;&nbsp;<span class="internal-nav"><a href="#ref60">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">61</td>
    								<td class="reference"><a name="fn61"></a>1 SCR 103 (1968).&nbsp;&nbsp;<span class="internal-nav"><a href="#ref61">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">62</td>
    								<td class="reference"><a name="fn62"></a><em>Robinson, Julian v. The Attorney General of Jamaica</em> (2019) JMFC Full 04 [“Robinson”]&nbsp;&nbsp;<span class="internal-nav"><a href="#ref62">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">63</td>
    								<td class="reference"><a name="fn63"></a><em>Robinson, supra</em>, para 247(B)(49).&nbsp;&nbsp;<span class="internal-nav"><a href="#ref63">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">64</td>
    								<td class="reference"><a name="fn64"></a>Principle 5, Necessity and Proportionality Principles, <em>International Principles on the Application of Human Rights to Communication Surveillance</em> (May, 2014), <a href="https://necessaryandproportionate.org/principles#principle5" target="_blank"><em>https://necessaryandproportionate.org/principles#principle5</em></a>.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref64">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">65</td>
    								<td class="reference"><a name="fn65"></a>See also <em>Robinson, supra</em>, para 247(B)(56).&nbsp;&nbsp;<span class="internal-nav"><a href="#ref65">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">66</td>
    								<td class="reference"><a name="fn66"></a>AccessNow, <em>supra</em>, 31.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref66">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">67</td>
    								<td class="reference"><a name="fn67"></a>Committee of Experts under the Chairmanship of Justice B.N. Srikrishna (2018) <a href="https://www.meity.gov.in/writereaddata/files/Data_Protection_Committee_Report.pdf" target="_blank"><em>https://www.meity.gov.in/writereaddata/files/Data_Protection_Committee_Report.pdf</em></a>, 53.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref67">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">68</td>
    								<td class="reference"><a name="fn68"></a>White Paper of the Committee of Experts on a Data Protection Framework for India (2018), <a href="https://meity.gov.in/writereaddata/files/white_paper_on_data_protection_in_india_18122017_final_v2.1.pdf" target="_blank"><em>https://meity.gov.in/writereaddata/files/white_paper_on_data_protection_in_india_18122017_final_v2.1.pdf</em></a>, 104-105&nbsp;&nbsp;<span class="internal-nav"><a href="#ref68">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">69</td>
    								<td class="reference"><a name="fn69"></a>See also Justice AP Shah Report, <em>supra</em>, 24; Debbie McElHill, “GDPR Data Retention Quick Guide”, <em>Data Protection Network</em> <a href="https://www.dpnetwork.org.uk/gdpr-data-retention-guide/" target="_blank"><em>https://www.dpnetwork.org.uk/gdpr-data-retention-guide/</em></a>.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref69">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">70</td>
    								<td class="reference"><a name="fn70"></a><em>S & Marper, supra</em>, paras 107, 114, 119.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref70">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">71</td>
    								<td class="reference"><a name="fn71"></a>Anmol Somanchi et al, “Well Done ABBA?,” 52(7) <em>EPW</em> (2017) <a href="https://www.epw.in/journal/2017/7/web-exclusives/well-done-abba.html" target="_blank"><em>https://www.epw.in/journal/2017/7/web-exclusives/well-done-abba.html</em></a>.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref71">&uarr;</em></a></span></td>
								</tr>

  								<tr>
    								<td class="number">72</td>
    								<td class="reference"><a name="fn72"></a>Geeta Pillai, “Need internet to buy PDS rations? Go climb a tree,” <em>The Times of India</em>, March 3, 2017, <a href="https://timesofindia.indiatimes.com/india/need-internet-to-buy-pds-rations-go-climb-a-tree/articleshow/57437975.cms" target="_blank"><em>https://timesofindia.indiatimes.com/india/need-internet-to-buy-pds-rations-go-climb-a-tree/articleshow/57437975.cms</em></a>.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref72">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">73</td>
    								<td class="reference"><a name="fn73"></a>Staff, “In Telangana, worn out fingerprints key reason behind 36% Aadhaar verification failure in key govt. scheme:Report,” <em>HuffPost</em>, April 7, 2017 <a href="https://www.huffingtonpost.in/2017/04/07/in-telangana-worn-out-fingerprints-behind-a-whopping-36-authen_a_22029773/?guccounter=1" target="_blank"><em>https://www.huffingtonpost.in/2017/04/07/in-telangana-worn-out-fingerprints-behind-a-whopping-36-authen_a_22029773/?guccounter=1</em></a>.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref73">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">74</td>
    								<td class="reference"><a name="fn74"></a>Gaurav Bhatnagar, “Testimonies Reveal how Aadhaar has Brought Pain, Exclusion to Poor,” <em>The Wire</em>, March 15, 2018, <a href="https://thewire.in/government/aadhaar-right-to-food-pain-exclusion" target="_blank"><em>https://thewire.in/government/aadhaar-right-to-food-pain-exclusion</em></a>.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref74">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">75</td>
    								<td class="reference"><a name="fn75"></a>Jahnavi Sen, “In Rural Jharkhand, Aadhaar Link to Welfare Schemes is Excluding the Most Needy” <em>The Wire</em>, September 26, 2018, <a href="https://thewire.in/government/jharkhand-aadhaar-pds-pensions" target="_blank"><em>https://thewire.in/government/jharkhand-aadhaar-pds-pensions</em></a>.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref75">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">76</td>
    								<td class="reference"><a name="fn76"></a>Subhashis Banerjee, Subodh Sharma, “An Offline Alternative for Aadhaar-based Authentication,” <em>Ideas for India</em>, September 24, 2018, <a href="https://www.ideasforindia.in/topics/productivity-innovation/an-%20offline-alternative-for-aadhaar-based-biometric-authentication.html" target="_blank"><em>https://www.ideasforindia.in/topics/productivity-innovation/an-%20offline-alternative-for-aadhaar-based-biometric-authentication.html</em></a>; Reetika Khera, “Aadhaar Bill Debate” Ideas for Change, <a href="https://www.ideasforindia.in/templates/i4ihome/images/author/Aadhaar-Bill-Debate-Reetika-Khera.pdf" target="_blank"><em>https://www.ideasforindia.in/templates/i4ihome/images/author/Aadhaar-Bill-Debate-Reetika-Khera.pdf</em></a>.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref76">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">77</td>
    								<td class="reference"><a name="fn77"></a>Olivia White et al., <em>Digital Identification: a Key to Inclusive Growth</em> (McKinsey Global Institute 2019) 7, <a href="https://www.mckinsey.com/~/media/McKinsey/Business%20Functions/McKinsey%20Digital/Our%20Insights/Digital%20identification%20A%20key%20to%20inclusive%20growth/MGI-Digital-identification-Report.ashx" target="_blank"><em>https://www.mckinsey.com/~/media/McKinsey/Business%20Functions/McKinsey%20Digital/Our%20Insights/Digital%20identification%20A%20key%20to%20inclusive%20growth/MGI-Digital-identification-Report.ashx</em></a> [“McKinsey”]&nbsp;&nbsp;<span class="internal-nav"><a href="#ref77">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">78</td>
    								<td class="reference"><a name="fn78"></a><em>Robinson, supra</em>, 205.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref78">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">79</td>
    								<td class="reference"><a name="fn79"></a>AccessNow, <em>supra</em>, 23-24.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref79">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">80</td>
    								<td class="reference"><a name="fn80"></a><em>Robinson, supra</em>, para 247(B)(19), (48), (52), 349.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref80">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">81</td>
    								<td class="reference"><a name="fn81"></a><em>Robinson, supra</em>, para 237.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref81">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">82</td>
    								<td class="reference"><a name="fn82"></a>McKinsey, <em>supra</em>.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref82">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">83</td>
    								<td class="reference"><a name="fn83"></a>edidiah Bracy, “Demystifying the Risk Based Approach” <em>The Privacy Adviser</em>, April 30, 2014 <a href="https://iapp.org/news/a/demystifying-the-risk-based-approach/" target="_blank"><em>https://iapp.org/news/a/demystifying-the-risk-based-approach/</em></a>.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref83">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">84</td>
    								<td class="reference"><a name="fn84"></a>CNIL, Methodology for Risk Management (2012) 7, <a href="https://www.cnil.fr/sites/default/files/typo/document/CNIL-ManagingPrivacyRisks-Methodology.pdf" target="_blank"><em>https://www.cnil.fr/sites/default/files/typo/document/CNIL-ManagingPrivacyRisks-Methodology.pdf</em></a>, [“CNIL”]. &nbsp;&nbsp;<span class="internal-nav"><a href="#ref84">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">85</td>
    								<td class="reference"><a name="fn85"></a>Fred Cate, “The Failure of Fair Information Practice Principles” in Winn, Jane K., ed. Consumer Protection in the Age of the 'Iinformation Economy'. London: Routledge, 2016.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref85">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">86</td>
    								<td class="reference"><a name="fn86"></a>CNIL, <em>supra</em>, at 8.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref86">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">87</td>
    								<td class="reference"><a name="fn87"></a>AccessNow, <em>supra</em>, at 31.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref87">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">88</td>
    								<td class="reference"><a name="fn88"></a>AccessNow, <em>supra</em>, at 27.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref88">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">89</td>
    								<td class="reference"><a name="fn89"></a>Dave Birch et al., “Digital Identity: Issue Analysis”, June 8, 2016, <a href="http://www.chyp.com/wp-content/uploads/2016/07/Digital-Identity-Issue-Analysis-Report.pdf" target="_blank"><em>http://www.chyp.com/wp-content/uploads/2016/07/Digital-Identity-Issue-Analysis-Report.pdf</em></a>.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref89">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">90</td>
    								<td class="reference"><a name="fn90"></a>National Research Council in Washington DC Report, “Biometric Recognition: Challenges and Opportunities, 2010, <a href="https://dataprivacylab.org/TIP/2011sept/Biometric.pdf" target="_blank"><em>https://dataprivacylab.org/TIP/2011sept/Biometric.pdf<a href="" target="_blank"><em>, 1.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref90">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">91</td>
    								<td class="reference"><a name="fn91"></a>ID4D WB, <em>supra</em>, 1; Mckinsey, <em>supra</em>, vi.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref91">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">92</td>
    								<td class="reference"><a name="fn92"></a><em>Robinson, supra</em>, para 247(B)(48), (53). See also Liu, <em>supra</em>, 36-54.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref92">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">93</td>
    								<td class="reference"><a name="fn93"></a>“Reducing False Positives without Increasing Regulatory Risks,” Oracle, last accessed August 1, 2019 <a href="https://www.oracle.com/technetwork/middleware/ows/documentation/ows-reducing-false-positives-wp-1864957.pdf" target="_blank"><em>https://www.oracle.com/technetwork/middleware/ows/documentation/ows-reducing-false-positives-wp-1864957.pdf</em></a> (webpage discontinued).&nbsp;&nbsp;<span class="internal-nav"><a href="#ref93">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">94</td>
    								<td class="reference"><a name="fn94"></a>ID4D WB, <em>supra</em>, 2.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref94">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">95</td>
    								<td class="reference"><a name="fn95"></a>Nikhil Pahwa, <em>supra</em>.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref95">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">96</td>
    								<td class="reference"><a name="fn96"></a>Nikhil Pahwa, <em>supra</em>.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref96">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">97</td>
    								<td class="reference"><a name="fn97"></a>ID4D WB, <em>supra</em>, 18.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref97">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">98</td>
    								<td class="reference"><a name="fn98"></a>AccessNow, supra, 9; Aili Vahatla, “Estonia Cancels Security Certificates of 11,100 electronic ID cards”, <em>ERR News</em>, June 1, 2018, <a href="https://news.err.ee/836259/estonia-cancels-security-certificates-of-11-100-electronic-id-cards" target="_blank"><em>https://news.err.ee/836259/estonia-cancels-security-certificates-of-11-100-electronic-id-cards</em></a>.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref98">&uarr;</em></a></span></td>
								</tr>
  								<tr>
    								<td class="number">99</td>
    								<td class="reference"><a name="fn99"></a>“What we learned from the eID card security risk?,” e-estonia,  last accessed January 22, 2019, <a href="https://e-estonia.com/card-security-risk/" target="_blank"><em>https://e-estonia.com/card-security-risk/</em></a>.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref99">&uarr;</em></a></span></td>
								</tr>



							</table>

						</div>
					</section>



				<!-- Divider -->
				<div id="divider"><hr /></div>

				<!-- Section -->
					<section>
						<header>
							<span class="image logo"><img src="images/CIS_Logo.jpg" alt="The Centre for Internet and Society (CIS), India" /></span>
						</header>
						<div class="content">
							<p>This website presents research undertaken by <a href="https://cis-india.org/" target="_blank">the Centre for Internet and Society, India</a> on appropriate design choices for digital identity frameworks, and their implications for both the sustainable development agenda as well for civil, social and economic rights. This research is supported by a <a href="https://www.omidyar.com/investees/centre-internet-and-society" target="_blank">grant</a> from <a href="https://www.omidyarnetwork.in/" target="_blank">Omidyar Network India</a>.<br /></p>
							<p>CIS is a non-profit organisation that undertakes interdisciplinary research on internet and digital technologies from policy and academic perspectives. Through its diverse initiatives, CIS explores, intervenes in, and advances contemporary discourse and regulatory practices around internet, technology, and society in India, and elsewhere.</p>
						</div>
					</section>

				<!-- Section -->
					<section>
						<header>
							<footer>

								<p>Copyright: <a href="https://cis-india.org" target="_blank">CIS, India</a>, 2019<br />
								License: <a href="https://creativecommons.org/licenses/by/4.0/" target="_blank">CC BY 4.0 International</a><br />
								Design: <a href="https://html5up.net/paradigm-shift" target="_blank">Paradigm Shift</a> by <a href="https://html5up.net" target="_blank">HTML5 UP</a><br />
								Fonts: <a href="https://fonts.google.com/specimen/Fira+Sans" target="_blank">Fira Sans</a> and <a href="https://fonts.google.com/specimen/IBM+Plex+Serif" target="_blank">IBM Plex Serif</a> by <a href="https://fonts.google.com/" target="_blank">Google Fonts</a><br />
								Hosted on <a href="https://github.com/cis-india/digitalid.design" target="_blank">GitHub</a></p>
							</footer>
						</header>
						<div class="content">
						</div>
					</section>
			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>
		<!-- Hypothesis -->
			<script type="application/json" class="js-hypothesis-config">
				{"showHighlights": false}
			</script>
			<script src="https://hypothes.is/embed.js" async></script>

	</body>
</html>
